---
title: "TP2_Maison"
author: "Zeyu CHEN"
date: "2018/10/1"
output: pdf_document
---

# I. Tests of significativity and model selection 
### a)
```{r , include=TRUE}

n<-100

X1 <- ((1:n)/n)^3
X2 <- ((1:n)/n)^4

#cbind column bind
#rbind rang bind
X<-cbind(X1,X2)
Y<-X%*%c(1,1)+rnorm(n)/4

res <- summary(lm(Y~X))

#on trouve que plus X1 et X2 sont proche, plus "significativity " de co-var sont bas
print(res)

print(res$coefficients)
print(res$coefficients[2,4])

reg1 <- lm(Y~X[,1])
print(summary(reg1))
reg2 <- lm(Y~X[,2])
print(summary(reg2))

#on remarque que si on modelise séparemment X1 et X2 , co-var sont tres significatif
```

### b)
```{r , include=TRUE}
cor(X[,1],X[,2])
# La corrélation entre X1 et X2 est très grande, donc ils ne sont pas très indépendants. C'est pourquoi notre model avec à la fois X1 et X2 en tant que covariables n'est pas bon.
```

# II. Model selection in a linear regression framework

### prework)
```{r , include=TRUE}
tab <- read.table("data/UsCrime.txt",header = TRUE)
nrow(tab)
#on peut rien voir sur ce plot
plot(tab)
library(corrplot)
cor(tab)

corrplot(cor(tab))

```
### A.Multiple regression model
```{r , include=TRUE}
reg <- lm("R~.",data=tab)
summary(reg)

X <- subset(tab,select= -R)
X <- as.matrix(data.frame(1,X))
coef <- as.vector( reg$coefficients)
Yp <-  X%*%coef
sum((tab$R-Yp)^2)/length(Yp)

```
### B.Model selection
#a backward)
```{r , include=TRUE}

?step()
regbackward <- step(reg,direction = 'backward')

summary(regbackward)
```
#b forward)
```{r , include=TRUE}
regforward <- step(lm(R~1,data = tab),list(upper=reg),direction = 'forward')
summary(regforward)
```
#c stepwise reg)
```{r , include=TRUE}
regboth <- step(reg,direction='both')
summary(regboth)
#For each step of stepwise regression, it will compare aslo the AIC for the vars who have been deleted 
```


# III. RIDGE and LASSO penalized regression

## A.Simulated data . Illustration

### a) Execute and comments the results using the following instructions
```{r , include=TRUE}
rm(list=ls())  
n<- 10000 
p<- 5

X <- matrix(rnorm(n*(p)),nrow = n,ncol = p)

#scale X is function whose method center and scale the column of a numeric matrix

X <-scale(X)*sqrt(n/(n-1))

# rev(1:p)  -> 5 4 3 2 1

#beta est un vector columne 50 40 30 20 10
beta<- matrix(10*rev(1:p),nrow = p,ncol=1)
print(beta)

# generate 10000 number generated by normal distribution 
epsi <- rnorm(n,1/n^2)

# it is our model y = bx + e , but its in formal of matrix
Y <- X%*%beta + epsi

Z <- cbind(Y,data.frame(X))

Z <- data.frame(Z)


```

### b)
```{r , include=TRUE}

library("glmnet")
sum(epsi)/length(epsi)
lmod <- lm(Y~X)
summary(lmod)
# the coefficients of X is equal  vector beta, the intercept is equal to E(epsi)

t(X)%*%Y/n


```

### c)
```{r , include=TRUE}
library("lars")
library("glmnet")

modlasso <- lars(X,Y,type = "lasso")
modlasso
attributes(modlasso)


modlasso$meanx
modlasso$normx
 nm <- dim(X)
  n <- nm[1]
one <- rep(1,n)


#modlasso$meanx store the l1 norm of each column Xi(ith column)
#modlasso$normx store the l2 norm of each column Xi(ith column)
```
### d)
```{r , include=TRUE}

par(mfrow=c(1,2))

plot(modlasso)

plot(c(modlasso$lambda,0),pch=16,type = "b",col="blue")

grid()
```
### e)
```{r , include=TRUE}
print(coef(modlasso))

modlasso$lambda

coef <- predict.lars(modlasso,X,type = "coefficients",mode="lambda",s=2000)

coeflasso <- coef$coefficients
coeflasso
par(mfrow=c(1,1))

barplot(coeflasso,main="lassso,l=1",col = "cyan")
```

B) Applications

```{r , include=TRUE}
tab <- read.table("data/usa_indicators.txt",sep = ";",header = TRUE)

```
b)
```{r , include=TRUE}
dim(tab)
tab
```
c)
```{r , include=TRUE}
#EN.ATM.CO2E.KT for CO2 emission
plot(tab$EN.ATM.CO2E.KT,pch=16,type = "b",col="blue")
```

d)
```{r , include=TRUE}

tab <- read.table("data/usa_indicators.txt",sep = ";",header = TRUE)
modCo2 <- lm(EN.ATM.CO2E.KT~.,tab)


```

RIDEGE)
a)
```{r , include=TRUE}
library(MASS)

help("lm.ridge")

```

b)
```{r , include=TRUE}
tab <- read.table("data/usa_indicators.txt",sep = ";",header = TRUE)
summary(scale(tab))
tabNoyear <- subset(tab,select=-Year)

tabNoyear <- as.data.frame(scale(tabNoyear))
resridge <- lm.ridge(EN.ATM.CO2E.KT ~.,tabNoyear,lambda = c(0,100))

coef(resridge)

resridge$coef

#coef(resridge)  values of the coefficients in the initial framework

#resridge$coef values of the coefficients in the ”rescaling framework”

```

c)
```{r , include=TRUE}

resridge <- lm.ridge(EN.ATM.CO2E.KT ~.,tabNoyear,lambda = seq(0,10,0.01))

plot(resridge$GCV)

plot(resridge)

coef(resridge)

which.min(resridge$GCV)

coefridge <- coef(lm.ridge(EN.ATM.CO2E.KT ~.,tabNoyear,lambda = 0.01))
```

d)
```{r , include=TRUE}

X<- subset(tabNoyear,select=-EN.ATM.CO2E.KT)

#We shoud add a column 1 for X
X<- data.frame(1,X)

Yridge <- as.matrix(X)%*%as.vector(coefridge) 

tab$EN.ATM.CO2E.KT

Yridge

sum((tabNoyear$EN.ATM.CO2E.KT -  Yridge)^2)/14
```

LASSO)
a)
```{r , include=TRUE}

Y <- tabNoyear$EN.ATM.CO2E.KT

X <- as.matrix(subset(tabNoyear,select=-EN.ATM.CO2E.KT))
scale(X)

reslasso <- lars(X,Y,type = "lasso")
par(mfrow=c(1,2))

plot(reslasso)

plot(c(reslasso$lambda,0),pch=16,type = "b",col="blue")

grid()


```

b)
```{r , include=TRUE}

#Pourquoi s = 0 , le resulta est comme ça? normallement , quand s=0, on aura
# les coef qui sont même que ceux de lm .
# parce que les observation sont trop petit.
tab <- read.table("data/usa_indicators.txt",sep = ";",header = TRUE)
tabNoyear <- subset(tab,select=-Year)

Y <- tabNoyear$EN.ATM.CO2E.KT
X <- as.matrix(subset(tabNoyear,select=-EN.ATM.CO2E.KT))
X <- scale(X)

reslasso <- lars(X,Y,type = "lasso")

coef <- predict.lars(reslasso,X,type ="coefficients",mode = "lambda",s=0)
barplot(coef$coefficients,main="lassso,l=1",col = "cyan")

coef<- predict.lars(reslasso,X,type = "coefficients",mode = "lambda",s=1000)
barplot(coef$coefficients,main="lassso,l=1",col = "cyan")

plot(coef$coefficients)
```


d)
```{r , include=TRUE}
pY <- predict.lars(reslasso,X,type = "fit",mode = "lambda",s=0.06)

sum((tabNoyear$EN.ATM.CO2E.KT - pY$fit)^2)/14

#e)
# on choisis lambda qui a quadratic error le plus petit 
```






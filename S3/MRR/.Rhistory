modRigeBH <- lm.ridge(medv~.,TabTrain,lambda = seq(0,10,0.01))
plot(modRigeBH$GCV)
plot(modRigeBH)
coef(modRigeBH)
#The index which correspond the smallest GCV is the best index of lambda
lambda <- which.min(modRigeBH$GCV)
coefridge <- coef(lm.ridge(medv~.,TabTrain,lambda = lambda))
coefridge
rm(list = ls())
library(mlbench)
data(BostonHousing)
BostonHousing$chas = as.numeric(BostonHousing$chas)
sub <- sample(nrow(BostonHousing), 0.6 * nrow(BostonHousing))
TabTrain <- BostonHousing[sub,]
TabTest <- BostonHousing[-sub,]
scale(as.matrix(TabTrain))
#Compute ridge regression model for different values of lambda starting from 0
#to 10 with an increment of 0.01 which is for finding the best lambda
modRigeBH <- lm.ridge(medv~.,TabTrain,lambda = seq(0,10,0.01))
plot(modRigeBH$GCV)
plot(modRigeBH)
coef(modRigeBH)
#The index which correspond the smallest GCV is the best index of lambda
lambda <- which.min(modRigeBH$GCV)
lambda
lambda+1
coefridge <- coef(lm.ridge(medv~.,TabTrain,lambda = lambda))
coefridge
rm(list = ls())
library(mlbench)
data(BostonHousing)
BostonHousing$chas = as.numeric(BostonHousing$chas)
sub <- sample(nrow(BostonHousing), 0.6 * nrow(BostonHousing))
TabTrain <- BostonHousing[sub,]
TabTest <- BostonHousing[-sub,]
scale(as.matrix(TabTrain))
#Compute ridge regression model for different values of lambda starting from 0
#to 10 with an increment of 0.01 which is for finding the best lambda
modRigeBH <- lm.ridge(medv~.,TabTrain,lambda = seq(0,10,0.01))
plot(modRigeBH$GCV)
plot(modRigeBH)
coef(modRigeBH)
#The index which correspond the smallest GCV is the best index of lambda
lambda <- which.min(modRigeBH$GCV)
lambda[1]
lambda[1]+1
coefridge <- coef(lm.ridge(medv~.,TabTrain,lambda = lambda))
coefridge
rm(list = ls())
library(mlbench)
data(BostonHousing)
BostonHousing$chas = as.numeric(BostonHousing$chas)
sub <- sample(nrow(BostonHousing), 0.6 * nrow(BostonHousing))
TabTrain <- BostonHousing[sub,]
TabTest <- BostonHousing[-sub,]
scale(as.matrix(TabTrain))
#Compute ridge regression model for different values of lambda starting from 0
#to 10 with an increment of 0.01 which is for finding the best lambda
modRigeBH <- lm.ridge(medv~.,TabTrain,lambda = seq(0,10,0.01))
plot(modRigeBH$GCV)
plot(modRigeBH)
coef(modRigeBH)
#The index which correspond the smallest GCV is the best index of lambda
lambda <- which.min(modRigeBH$GCV)
lambda$first
rm(list = ls())
library(mlbench)
data(BostonHousing)
BostonHousing$chas = as.numeric(BostonHousing$chas)
sub <- sample(nrow(BostonHousing), 0.6 * nrow(BostonHousing))
TabTrain <- BostonHousing[sub,]
TabTest <- BostonHousing[-sub,]
scale(as.matrix(TabTrain))
#Compute ridge regression model for different values of lambda starting from 0
#to 10 with an increment of 0.01 which is for finding the best lambda
modRigeBH <- lm.ridge(medv~.,TabTrain,lambda = seq(0,10,0.01))
plot(modRigeBH$GCV)
plot(modRigeBH)
coef(modRigeBH)
#The index which correspond the smallest GCV is the best index of lambda
lambda <- which.min(modRigeBH$GCV)
type(lambda)
rm(list = ls())
library(mlbench)
data(BostonHousing)
BostonHousing$chas = as.numeric(BostonHousing$chas)
sub <- sample(nrow(BostonHousing), 0.6 * nrow(BostonHousing))
TabTrain <- BostonHousing[sub,]
TabTest <- BostonHousing[-sub,]
scale(as.matrix(TabTrain))
#Compute ridge regression model for different values of lambda starting from 0
#to 10 with an increment of 0.01 which is for finding the best lambda
modRigeBH <- lm.ridge(medv~.,TabTrain,lambda = seq(0,10,0.01))
plot(modRigeBH$GCV)
plot(modRigeBH)
coef(modRigeBH)
#The index which correspond the smallest GCV is the best index of lambda
lambda <- which.min(modRigeBH$GCV)
class(lambda)
lambda[1]+1
coefridge <- coef(lm.ridge(medv~.,TabTrain,lambda = lambda))
coefridge
rm(list = ls())
library(mlbench)
data(BostonHousing)
BostonHousing$chas = as.numeric(BostonHousing$chas)
sub <- sample(nrow(BostonHousing), 0.6 * nrow(BostonHousing))
TabTrain <- BostonHousing[sub,]
TabTest <- BostonHousing[-sub,]
scale(as.matrix(TabTrain))
#Compute ridge regression model for different values of lambda starting from 0
#to 10 with an increment of 0.01 which is for finding the best lambda
modRigeBH <- lm.ridge(medv~.,TabTrain,lambda = seq(0,10,0.01))
plot(modRigeBH$GCV)
plot(modRigeBH)
coef(modRigeBH)
#The index which correspond the smallest GCV is the best index of lambda
lambda <- which.min(modRigeBH$GCV)
mode(lambda)
lambda[1]+1
coefridge <- coef(lm.ridge(medv~.,TabTrain,lambda = lambda))
coefridge
rm(list = ls())
library(mlbench)
data(BostonHousing)
BostonHousing$chas = as.numeric(BostonHousing$chas)
sub <- sample(nrow(BostonHousing), 0.6 * nrow(BostonHousing))
TabTrain <- BostonHousing[sub,]
TabTest <- BostonHousing[-sub,]
scale(as.matrix(TabTrain))
#Compute ridge regression model for different values of lambda starting from 0
#to 10 with an increment of 0.01 which is for finding the best lambda
modRigeBH <- lm.ridge(medv~.,TabTrain,lambda = seq(0,10,0.01))
plot(modRigeBH$GCV)
plot(modRigeBH)
coef(modRigeBH)
#The index which correspond the smallest GCV is the best index of lambda
lambda <- which.min(modRigeBH$GCV)
mode(lambda)
coefridge <- coef(lm.ridge(medv~.,TabTrain,lambda = lambda))
coefridge
rm(list = ls())
library(mlbench)
data(BostonHousing)
BostonHousing$chas = as.numeric(BostonHousing$chas)
sub <- sample(nrow(BostonHousing), 0.6 * nrow(BostonHousing))
TabTrain <- BostonHousing[sub,]
TabTest <- BostonHousing[-sub,]
scale(as.matrix(TabTrain))
#Compute ridge regression model for different values of lambda starting from 0
#to 10 with an increment of 0.01 which is for finding the best lambda
modRigeBH <- lm.ridge(medv~.,TabTrain,lambda = seq(0,10,0.01))
plot(modRigeBH$GCV)
plot(modRigeBH)
coef(modRigeBH)
#The index which correspond the smallest GCV is the best index of lambda
lambda <- which.min(modRigeBH$GCV)
mode(lambda)
lambda
coefridge <- coef(lm.ridge(medv~.,TabTrain,lambda = lambda))
coefridge
lambda+1
lambda
lambda
rm(list = ls())
library(mlbench)
data(BostonHousing)
BostonHousing$chas = as.numeric(BostonHousing$chas)
sub <- sample(nrow(BostonHousing), 0.6 * nrow(BostonHousing))
TabTrain <- BostonHousing[sub,]
TabTest <- BostonHousing[-sub,]
scale(as.matrix(TabTrain))
#Compute ridge regression model for different values of lambda starting from 0
#to 10 with an increment of 0.01 which is for finding the best lambda
modRigeBH <- lm.ridge(medv~.,TabTrain,lambda = seq(0,10,0.01))
plot(modRigeBH$GCV)
plot(modRigeBH)
coef(modRigeBH)
#The index which correspond the smallest GCV is the best index of lambda
lambda <- which.min(modRigeBH$GCV)
lambda
lambda+1
coefridge <- coef(lm.ridge(medv~.,TabTrain,lambda = lambda))
coefridge
lambda$
dd
rm(list = ls())
library(mlbench)
data(BostonHousing)
BostonHousing$chas = as.numeric(BostonHousing$chas)
sub <- sample(nrow(BostonHousing), 0.6 * nrow(BostonHousing))
TabTrain <- BostonHousing[sub,]
TabTest <- BostonHousing[-sub,]
scale(as.matrix(TabTrain))
#Compute ridge regression model for different values of lambda starting from 0
#to 10 with an increment of 0.01 which is for finding the best lambda
modRigeBH <- lm.ridge(medv~.,TabTrain,lambda = seq(0,10,0.01))
plot(modRigeBH$GCV)
plot(modRigeBH)
coef(modRigeBH)
#The index which correspond the smallest GCV is the best index of lambda
lambda <- which.min(modRigeBH$GCV)
lambda
attributes(lambda)
coefridge <- coef(lm.ridge(medv~.,TabTrain,lambda = lambda))
coefridge
rm(list = ls())
library(mlbench)
data(BostonHousing)
BostonHousing$chas = as.numeric(BostonHousing$chas)
sub <- sample(nrow(BostonHousing), 0.6 * nrow(BostonHousing))
TabTrain <- BostonHousing[sub,]
TabTest <- BostonHousing[-sub,]
scale(as.matrix(TabTrain))
#Compute ridge regression model for different values of lambda starting from 0
#to 10 with an increment of 0.01 which is for finding the best lambda
modRigeBH <- lm.ridge(medv~.,TabTrain,lambda = seq(0,10,0.01))
plot(modRigeBH$GCV)
plot(modRigeBH)
coef(modRigeBH)
#The index which correspond the smallest GCV is the best index of lambda
lambda <- which.min(modRigeBH$GCV)
lambda$names
rm(list = ls())
library(mlbench)
data(BostonHousing)
BostonHousing$chas = as.numeric(BostonHousing$chas)
sub <- sample(nrow(BostonHousing), 0.6 * nrow(BostonHousing))
TabTrain <- BostonHousing[sub,]
TabTest <- BostonHousing[-sub,]
scale(as.matrix(TabTrain))
#Compute ridge regression model for different values of lambda starting from 0
#to 10 with an increment of 0.01 which is for finding the best lambda
modRigeBH <- lm.ridge(medv~.,TabTrain,lambda = seq(0,10,0.01))
plot(modRigeBH$GCV)
plot(modRigeBH)
coef(modRigeBH)
#The index which correspond the smallest GCV is the best index of lambda
lambda <- which.min(modRigeBH$GCV)
modRigeBH$GCV[lambda]
coefridge <- coef(lm.ridge(medv~.,TabTrain,lambda = lambda))
coefridge
modRigeBH$GCV[lambda]
modRigeBH$GCV
rm(list = ls())
library(mlbench)
data(BostonHousing)
BostonHousing$chas = as.numeric(BostonHousing$chas)
sub <- sample(nrow(BostonHousing), 0.6 * nrow(BostonHousing))
TabTrain <- BostonHousing[sub,]
TabTest <- BostonHousing[-sub,]
scale(as.matrix(TabTrain))
#Compute ridge regression model for different values of lambda starting from 0
#to 10 with an increment of 0.01 which is for finding the best lambda
modRigeBH <- lm.ridge(medv~.,TabTrain,lambda = seq(0,10,0.01))
plot(modRigeBH$GCV)
plot(modRigeBH)
coef(modRigeBH)
#The index which correspond the smallest GCV is the best index of lambda
lambda <- which.min(modRigeBH$GCV)
coefridge <- coef(lm.ridge(medv~.,TabTrain,lambda = modRigeBH$GCV[lambda]))
coefridge
rm(list = ls())
library(mlbench)
data(BostonHousing)
BostonHousing$chas = as.numeric(BostonHousing$chas)
sub <- sample(nrow(BostonHousing), 0.6 * nrow(BostonHousing))
TabTrain <- BostonHousing[sub,]
TabTest <- BostonHousing[-sub,]
scale(as.matrix(TabTrain))
#Compute ridge regression model for different values of lambda starting from 0
#to 10 with an increment of 0.01 which is for finding the best lambda
modRigeBH <- lm.ridge(medv~.,TabTrain,lambda = seq(0,10,0.01))
plot(modRigeBH$GCV)
plot(modRigeBH)
coef(modRigeBH)
#The index which correspond the smallest GCV is the best index of lambda
indexlambda <- which.min(modRigeBH$GCV)
coefridge <- coef(lm.ridge(medv~.,TabTrain,lambda = modRigeBH$GCV[indexlambda]))
data.frame(coefridge)
data.frame(coefRidge)
rm(list = ls())
library(mlbench)
data(BostonHousing)
BostonHousing$chas = as.numeric(BostonHousing$chas)
sub <- sample(nrow(BostonHousing), 0.6 * nrow(BostonHousing))
TabTrain <- BostonHousing[sub,]
TabTest <- BostonHousing[-sub,]
scale(as.matrix(TabTrain))
#Compute ridge regression model for different values of lambda starting from 0
#to 10 with an increment of 0.01 which is for finding the best lambda
modRigeBH <- lm.ridge(medv~.,TabTrain,lambda = seq(0,10,0.01))
plot(modRigeBH$GCV)
plot(modRigeBH)
coef(modRigeBH)
#The index which correspond the smallest GCV is the best index of lambda
indexlambda <- which.min(modRigeBH$GCV)
coefridge <- coef(lm.ridge(medv~.,TabTrain,lambda = modRigeBH$GCV[indexlambda]))
data.frame(coefridge)
rm(list = ls())
library(mlbench)
data(BostonHousing)
BostonHousing$chas = as.numeric(BostonHousing$chas)
sub <- sample(nrow(BostonHousing), 0.6 * nrow(BostonHousing))
TabTrain <- BostonHousing[sub,]
TabTest <- BostonHousing[-sub,]
scale(as.matrix(TabTrain))
#Compute ridge regression model for different values of lambda starting from 0
#to 10 with an increment of 0.01 which is for finding the best lambda
modRigeBH <- lm.ridge(medv~.,TabTrain,lambda = seq(0,10,0.01))
plot(modRigeBH$GCV)
plot(modRigeBH)
coef(modRigeBH)
#The index which correspond the smallest GCV is the best index of lambda
indexlambda <- which.min(modRigeBH$GCV)
coefridge <- coef(lm.ridge(medv~.,TabTrain,lambda = modRigeBH$GCV[indexlambda]))
#finnally , the coefs that we found by using Ridge is here
data.frame(coefridge)
library(mlbench)
library(lars)
library(dplyr)
data(BostonHousing)
BostonHousing$chas = as.numeric(BostonHousing$chas)
#split data
sub <- sample(nrow(BostonHousing), 0.75 * nrow(BostonHousing))
TabTrain <- BostonHousing[sub,]
TabTest <- BostonHousing[-sub,]
Ytrain = TabTrain$medv
Ytest = TabTest$medv
Xtrain = as.matrix(scale(select(TabTrain,-medv)))
Xtest = as.matrix(scale(select(TabTest,-medv)))
#Xtrain = as.matrix(select(TabTrain,-medv))
#Xtest =as.matrix(select(TabTest,-medv))
#Lasso regression
modlasso = lars(Xtrain, Ytrain, type = "lasso")
plot(modlasso)
plot(modlasso$lambda)
pY = predict.lars(modlasso, Xtest, type = "fit", mode = "lambda", s=modlasso$lambda[1])
MSElasso = mean((Ytest - pY$fit)^2)
lambda = 1
for (i in 2:length(modlasso$lambda)){
pY = predict.lars(modlasso, Xtest, type = "fit", mode = "lambda", s=modlasso$lambda[i])
newMSElasso = mean((Ytest - pY$fit)^2)
if (MSElasso > newMSElasso){
MSElasso = newMSElasso
lambda = i
}
}
lambda #index of lambda which gives the smallest mean square error
MSElasso
modlasso$lambda[lambda]
coef = predict.lars(modlasso, Xtest, type = "coefficients", mode = "lambda", s = modlasso$lambda[lambda])
coef$coefficients #coefficients obtained for this lambda
pY = predict.lars(modlasso, Xtest, type = "fit", mode = "lambda", s=modlasso$lambda[lambda])
data.frame(sum((Ytest = TabTest$medv - pY$fit)^2)/14)
?data.frame
library(mlbench)
library(lars)
library(dplyr)
data(BostonHousing)
BostonHousing$chas = as.numeric(BostonHousing$chas)
#split data
sub <- sample(nrow(BostonHousing), 0.75 * nrow(BostonHousing))
TabTrain <- BostonHousing[sub,]
TabTest <- BostonHousing[-sub,]
Ytrain = TabTrain$medv
Ytest = TabTest$medv
Xtrain = as.matrix(scale(select(TabTrain,-medv)))
Xtest = as.matrix(scale(select(TabTest,-medv)))
#Xtrain = as.matrix(select(TabTrain,-medv))
#Xtest =as.matrix(select(TabTest,-medv))
#Lasso regression
modlasso = lars(Xtrain, Ytrain, type = "lasso")
plot(modlasso)
plot(modlasso$lambda)
pY = predict.lars(modlasso, Xtest, type = "fit", mode = "lambda", s=modlasso$lambda[1])
MSElasso = mean((Ytest - pY$fit)^2)
lambda = 1
for (i in 2:length(modlasso$lambda)){
pY = predict.lars(modlasso, Xtest, type = "fit", mode = "lambda", s=modlasso$lambda[i])
newMSElasso = mean((Ytest - pY$fit)^2)
if (MSElasso > newMSElasso){
MSElasso = newMSElasso
lambda = i
}
}
lambda #index of lambda which gives the smallest mean square error
MSElasso
modlasso$lambda[lambda]
coef = predict.lars(modlasso, Xtest, type = "coefficients", mode = "lambda", s = modlasso$lambda[lambda])
coef$coefficients #coefficients obtained for this lambda
pY = predict.lars(modlasso, Xtest, type = "fit", mode = "lambda", s=modlasso$lambda[lambda])
RSS <- data.frame(sum((Ytest = TabTest$medv - pY$fit)^2)/14)
names(RSS) <- "eee"
library(mlbench)
library(lars)
library(dplyr)
data(BostonHousing)
BostonHousing$chas = as.numeric(BostonHousing$chas)
#split data
sub <- sample(nrow(BostonHousing), 0.75 * nrow(BostonHousing))
TabTrain <- BostonHousing[sub,]
TabTest <- BostonHousing[-sub,]
Ytrain = TabTrain$medv
Ytest = TabTest$medv
Xtrain = as.matrix(scale(select(TabTrain,-medv)))
Xtest = as.matrix(scale(select(TabTest,-medv)))
#Xtrain = as.matrix(select(TabTrain,-medv))
#Xtest =as.matrix(select(TabTest,-medv))
#Lasso regression
modlasso = lars(Xtrain, Ytrain, type = "lasso")
plot(modlasso)
plot(modlasso$lambda)
pY = predict.lars(modlasso, Xtest, type = "fit", mode = "lambda", s=modlasso$lambda[1])
MSElasso = mean((Ytest - pY$fit)^2)
lambda = 1
for (i in 2:length(modlasso$lambda)){
pY = predict.lars(modlasso, Xtest, type = "fit", mode = "lambda", s=modlasso$lambda[i])
newMSElasso = mean((Ytest - pY$fit)^2)
if (MSElasso > newMSElasso){
MSElasso = newMSElasso
lambda = i
}
}
lambda #index of lambda which gives the smallest mean square error
MSElasso
modlasso$lambda[lambda]
coef = predict.lars(modlasso, Xtest, type = "coefficients", mode = "lambda", s = modlasso$lambda[lambda])
coef$coefficients #coefficients obtained for this lambda
pY = predict.lars(modlasso, Xtest, type = "fit", mode = "lambda", s=modlasso$lambda[lambda])
RSS <- data.frame(sum((Ytest = TabTest$medv - pY$fit)^2)/14)
names(RSS) <- "eee"
RSS
library(mlbench)
library(lars)
library(dplyr)
data(BostonHousing)
BostonHousing$chas = as.numeric(BostonHousing$chas)
#split data
sub <- sample(nrow(BostonHousing), 0.75 * nrow(BostonHousing))
TabTrain <- BostonHousing[sub,]
TabTest <- BostonHousing[-sub,]
Ytrain = TabTrain$medv
Ytest = TabTest$medv
Xtrain = as.matrix(scale(select(TabTrain,-medv)))
Xtest = as.matrix(scale(select(TabTest,-medv)))
#Xtrain = as.matrix(select(TabTrain,-medv))
#Xtest =as.matrix(select(TabTest,-medv))
#Lasso regression
modlasso = lars(Xtrain, Ytrain, type = "lasso")
plot(modlasso)
plot(modlasso$lambda)
pY = predict.lars(modlasso, Xtest, type = "fit", mode = "lambda", s=modlasso$lambda[1])
MSElasso = mean((Ytest - pY$fit)^2)
lambda = 1
for (i in 2:length(modlasso$lambda)){
pY = predict.lars(modlasso, Xtest, type = "fit", mode = "lambda", s=modlasso$lambda[i])
newMSElasso = mean((Ytest - pY$fit)^2)
if (MSElasso > newMSElasso){
MSElasso = newMSElasso
lambda = i
}
}
lambda #index of lambda which gives the smallest mean square error
MSElasso
modlasso$lambda[lambda]
coef = predict.lars(modlasso, Xtest, type = "coefficients", mode = "lambda", s = modlasso$lambda[lambda])
coef$coefficients #coefficients obtained for this lambda
pY = predict.lars(modlasso, Xtest, type = "fit", mode = "lambda", s=modlasso$lambda[lambda])
RSS <- data.frame(sum((Ytest = TabTest$medv - pY$fit)^2)/14)
names(RSS) <- "RSS by Lasso"
RSS
library(mlbench)
library(lars)
library(dplyr)
data(BostonHousing)
BostonHousing$chas = as.numeric(BostonHousing$chas)
#split data
sub <- sample(nrow(BostonHousing), 0.75 * nrow(BostonHousing))
TabTrain <- BostonHousing[sub,]
TabTest <- BostonHousing[-sub,]
Ytrain = TabTrain$medv
Ytest = TabTest$medv
Xtrain = as.matrix(scale(select(TabTrain,-medv)))
Xtest = as.matrix(scale(select(TabTest,-medv)))
#Xtrain = as.matrix(select(TabTrain,-medv))
#Xtest =as.matrix(select(TabTest,-medv))
#Lasso regression
modlasso = lars(Xtrain, Ytrain, type = "lasso")
plot(modlasso)
plot(modlasso$lambda)
pY = predict.lars(modlasso, Xtest, type = "fit", mode = "lambda", s=modlasso$lambda[1])
MSElasso = mean((Ytest - pY$fit)^2)
lambda = 1
for (i in 2:length(modlasso$lambda)){
pY = predict.lars(modlasso, Xtest, type = "fit", mode = "lambda", s=modlasso$lambda[i])
newMSElasso = mean((Ytest - pY$fit)^2)
if (MSElasso > newMSElasso){
MSElasso = newMSElasso
lambda = i
}
}
lambda #index of lambda which gives the smallest mean square error
MSElasso
modlasso$lambda[lambda]
coef = predict.lars(modlasso, Xtest, type = "coefficients", mode = "lambda", s = modlasso$lambda[lambda])
coef$coefficients #coefficients obtained for this lambda
pY = predict.lars(modlasso, Xtest, type = "fit", mode = "lambda", s=modlasso$lambda[lambda])
RSS <- data.frame(sum((Ytest = TabTest$medv - pY$fit)^2)/14)
names(RSS) <- "RSS by Lasso"
row.names(RSS) <- "RSS"
RSS

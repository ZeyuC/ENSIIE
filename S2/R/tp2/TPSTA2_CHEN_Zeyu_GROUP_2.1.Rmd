---
title: "TPSTAT_CHEN_Zeyu_GROUP_2.1"
author: "Zeyu"
date: "2018/3/5"
output:
  pdf_document:
    latex_engine: xelatex
  word_document: default
---
### 1.Echantillon, Théorème Central Limite, Estimation Monte Carlo
###<--1-->

```{r , include=TRUE}
# N est la fois d'échantillons
# f est la fonction qu'on va simuler
# n taille d'échantillon, u est mu, o est sigma 
sim.fun <-function (N,f,n,u,o)   
  { 
    sample<-1:N
    for (i in 1:N) { 
        sample[i] <-f(n,u,o)
     } 
    return(sample) 
} 

moyenne=function(n,mu,sigma)
  {
  r=rnorm(n,mu,sigma);
  return(mean(r));
} 

varience=function(n,mu,sigma)
  {
  r=rnorm(n,mu,sigma);
  return(var(r));
} 
m1 = sim.fun(1000,moyenne,5,1,2)
m2 = sim.fun(1000,moyenne,30,1,2) 
m3 = sim.fun(1000,moyenne,100,1,2)
v1 = sim.fun(1000,varience,5,1,2) 
v2 = sim.fun(1000,varience,30,1,2)
v3 = sim.fun(1000,varience,100,1,2)
hist(m1,freq =T ,main ="moy_emp_de_gaus_taille_5")
hist(m2,freq =T ,main ="moy_emp_de_gaus_taille_30")
hist(m3,freq =T ,main ="moy_emp_de_gaus_taille_100")

renormalisation<-function(N,moyenne_epq,n,mu,sigma)
{
  for ( i in 1:N)
  {
    moyenne_epq[i]=(moyenne_epq[i]-mu)/(sigma/sqrt(n));
  }
  return(moyenne_epq)
}

u1 = renormalisation(1000,m1,5,1,2);
u2 = renormalisation(1000,m2,30,1,2);
u3 = renormalisation(1000,m3,100,1,2);
hist(u1,freq =T ,main ="nor_de_taille_5")
hist(u2,freq =T ,main ="nor_de_taille_30")
hist(u3,freq =T ,main ="nor_de_taille_100")

```

###<--2-->
```{r , include=TRUE}
library(rmutil)
# N est la fois d'échantillons
# f est la fonction qu'on va simuler
# n taille d'échantillon  
sim.fun <-function (N,f,n,m,s)   
  { 
    sample<-1:N
    for (i in 1:N) { 
        sample[i] <-f(n,m,s)
     } 
    return(sample) 
} 
moyenne=function(n,m,s)
  {
  r=rpareto(n,m,s);
  return(mean(r));
} 

varience=function(n,m,s)
  {
  r=rpareto(n,m,s);
  return(var(r));
} 
m1 = sim.fun(1000,moyenne,5,1,3)
m2 = sim.fun(1000,moyenne,30,1,3) 
m3 = sim.fun(1000,moyenne,100,1,3)
v1 = sim.fun(1000,varience,5,1,3) 
v2 = sim.fun(1000,varience,30,1,3)
v3 = sim.fun(1000,varience,100,1,3)
hist(m1,freq =T ,xlim=c(0,2),main ="moy_emp_de_paro_taille_5")
hist(m2,freq =T ,xlim=c(0,2),main ="moy_emp_de_paro_taille_30")
hist(m3,freq =T ,xlim=c(0,2),main ="moy_emp_de_paro_taille_100")  
renormalisation<-function(N,moy_emp,n,mu,sigma)
{
  for ( i in 1:N)
  {
    moy_emp[i]=(moy_emp[i]-mu)/(sigma/sqrt(n));
  }
  return(moy_emp)
}
```
$${a}_{n}=m,{b}_{n}=s$$
```{r , include=TRUE}
u1 = renormalisation(1000,m1,5,1,3);
u2 = renormalisation(1000,m2,30,1,3);
u3 = renormalisation(1000,m3,100,1,3);
hist(u1,freq =T ,xlim=c(-5,5),main ="paro_de_taille_5")
hist(u2,freq =T ,xlim=c(-5,5),main ="paro_de_taille_30")
hist(u3,freq =T ,xlim=c(-5,5),main ="paro_de_taille_100")
  
```


###<--3-->

```{r , include=TRUE}
# N est la fois d'échantillons
# f est la fonction qu'on va simuler
# n taille d'échantillon 
sim.fun <-function (N,f,n,lamda)   
  { 
    sample<-1:N
    for (i in 1:N) { 
        sample[i] <-f(n,lamda)
     } 
    return(sample) 
} 
moyenne=function(n,lamda)
  {
  r=rpois(n,lamda);
  return(mean(r));
} 

varience=function(n,lamda)
  {
  r=rpois(n,lamda);
  return(var(r));
} 
m1 = sim.fun(1000,moyenne,5,1)
m2 = sim.fun(1000,moyenne,30,1) 
m3 = sim.fun(1000,moyenne,100,1)
v1 = sim.fun(1000,varience,5,1) 
v2 = sim.fun(1000,varience,30,1)
v3 = sim.fun(1000,varience,100,1)
hist(m1,freq =T ,main ="moy_emp_de_pois_taille_5")
hist(m2,freq =T ,main ="moy_emp_de_pois_taille_30")
hist(m3,freq =T ,main ="moy_emp_de_pois_taille_100")
renormalisation<-function(N,moy_emp,n,mu,sigma)
{
  for ( i in 1:N)
  {
    moy_emp[i]=(moy_emp[i]-mu)/(sigma/sqrt(n));
  }
  return(moy_emp)
}

u1 = renormalisation(1000,m1,5,1,1);
u2 = renormalisation(1000,m2,30,1,1);
u3 = renormalisation(1000,m3,100,1,1);
hist(u1,freq =T ,main ="nor_de_taille_5")
hist(u2,freq =T ,main ="nor_de_taille_30")
hist(u3,freq =T ,main ="nor_de_taille_100")

```


### <--4-->

quand on fais N fois échantillon (x1,x2..xn) iid de n'importe quel loi, on trouve que toute moyenne $\bar {X}_{n}$ des échantillons tends vers une variable aléatoire gausienne. En plus, plus N est grand, plus la distribution d'échantillonage est similaire à gaussienne avec $\mu$ = E(x) var = var(x).
Par ailleurs, $$ Z_{n}=\frac{\bar {X}_{n}-\mu}{\sigma/ \sqrt{n}}$$ $$ \lim_{n \to \infty}P(Z_{n}\leq z)=\Phi(z)$$
$\Phi(z)$ suis la **loi normale centrée réduite** $\cal{N}\rm (0,1)$

### 2.Moyenne et dispersion

### <--1-->

l'inégalité de Chebychef dans les cas Gaussien est 
$${P}(|{X}-\mu|\ge k\sigma)\le \frac{1}{k^2}$$
l'inégalité de Chebychef dans les cas Poisson est 
$${P}(|{X}-\lambda|\ge k\lambda)\le \frac{1}{k^2}$$
parce que l'espérence et la varience de Poison sont tous $\lambda$


### <--2-->
### 2.a
$${P}(|{X}-\mu|\ge \sigma)\ = E({I}_{|X-\mu|\ge k\sigma})$$
avec $I_{A}$ est une fonction indicatrice

### 2.b
```{r , include=TRUE}
#calcule le borne par Monte Carlo
#parma r une échantillon qu'on a obeteun,n la taille,mu espérence
#return la probabilté de déviation d'une v.a de sa moyenne 
library(rmutil)
  Prob_devia <- function(n,r,mu,delta)
  {
    res<- 0
    for (i in 1:n) {
        if (abs(r[i]-mu) >= delta)
          res<-res+1
    }
    return(res/n)
  }
  r1<-rnorm(100000,mean = 1,sd = 1)
  r2<-rpareto(10000,m=1,s=2)
  r3<-rpois(10000,lambda=1)
  

```
### On calcule par  la fonction P_deviation les probabilités de déviation d'une v.a de sa moyenne
Dans le cas Gaussien avec mean=1 sd=1 ${P}(|{X}-1|\ge 2)$
```{r , include=TRUE}
list("Gaussien "= Prob_devia(100000,r1,1,2))
```
Dans le cas Pareto avec m=1 s=1 ${P}(|{X}-1|\ge 2)=$
```{r , include=TRUE}
list("Pareto"=Prob_devia(10000,r2,1,2))
```

Dans le cas Poisson avec lambda=1${P}(|{X}-1|\ge 2)=$

```{r , include=TRUE}
list("Poisson"=Prob_devia(10000,r3,1,2))
```
La précison de cette estimation est cinq

### 2.c
Remarque:
Quand on prend un échantillon avec taille N, ${V[\bar X_{N}]}=\frac{1}{N}V[X]$
```{r , include=TRUE}

Borne_Chebcf <- function (delta,sigma)
{
  return ((sigma/delta)**2);
}
```


Quand le cas Gaussien avec ${\delta=1 \;\;\mu=0 \;\; \sigma=1}$
```{r , include=TRUE}
x1=Borne_Chebcf(1,1);
r= rnorm(10000,0,1)
x2=Prob_devia(10000,r,0,1);
list("Chebychev"=x1,"Monte Carlo"=x2) 
```

Quand le cas Gaussien avec ${\delta=1 \;\;\mu=0 \;\; \sigma=0.5}$
```{r , include=TRUE}
x1=Borne_Chebcf(1,0.5);
r= rnorm(10000,0,0.5)
x2=Prob_devia(10000,r1,0,1);
list("Chebychev"=x1,"Monte Carlo"=x2) 
```

Quand le cas Gaussien avec ${\delta=2 \;\;\mu=1 \;\; \sigma=1}$
```{r , include=TRUE}
x1=Borne_Chebcf(2,1);
r= rnorm(10000,1,1)
x2=Prob_devia(10000,r1,1,1);
list("Chebychev"=x1,"Monte Carlo"=x2) 
```

Quand le cas Poisson avec ${\delta=2 \;\; \lambda=1}$
```{r , include=TRUE}
x1=Borne_Chebcf(2,1);
r= rpois(10000,1)
x2=Prob_devia(10000,r,1,2);
list("Chebychev"=x1,"Monte Carlo"=x2)
```

Quand le cas Poisson avec ${\delta=2 \;\;  \lambda=0.5}$
```{r , include=TRUE}
x1=Borne_Chebcf(2,0.5);
r= rpois(10000,0.5)
x2=Prob_devia(10000,r,0.5,2);
list("Chebychev"=x1, "Monte Carl" = x2)
```

###d
Le cas Gaussien $X \sim N(\mu, \sigma)$
$P(X \ge t)  \le  exp(-\frac{(t-\mu)^2}{2\sigma^2})$ 
Donc : $P(X-\mu\ge \delta)  \le  exp(-\frac{\delta^2}{2\sigma^2})$ car ici $t = \mu+\delta$

```{r , include=TRUE}
Borne_Chernoff_Gaus<- function(sigma,delta)
{
  borne= exp(-(delta**2)/(2*(sigma**2)));
  return (borne);
}

Prob_devia_sans_abs <- function(n,r,mu,delta)
  {
    res<- 0
    for (i in 1:n) {
        if ((r[i]-mu) >= delta)
          res<-res+1
    }
    return(res/n)
  }

```
Quand le cas Gaussien avec ${\delta=1 \;\;\mu=0 \;\; \sigma=1}$

```{r , include=TRUE}
x1 = Borne_Chernoff_Gaus(1,1)
r= rnorm(10000,0,1)
x2=Prob_devia_sans_abs(10000,r,0,1);
list("Chernoff"=x1,"Monte Carlo"=x2) 
```
Quand le cas Gaussien avec ${\delta=2 \;\;\mu=1 \;\; \sigma=3}$
```{r , include=TRUE}
x1 = Borne_Chernoff_Gaus(3,2)
r= rnorm(10000,1,3)
x2=Prob_devia_sans_abs(10000,r,1,2);
list("Chernoff"=x1,"Monte Carlo"=x2) 
```
Quand le cas Gaussien avec ${\delta=3 \;\;\mu=1 \;\; \sigma=1}$
```{r , include=TRUE}
x1 = Borne_Chernoff_Gaus(1,3)
r= rnorm(10000,1,1)
x2=Prob_devia_sans_abs(10000,r,1,3);
list("Chernoff"=x1,"Monte Carlo"=x2) 
```

remarque:
$P(X \ge t)  \le  exp(-\mu h(\frac{t}{\mu})$ avec
$h(x)=(1+x)log(1+x)-x$

Quand le cas Poisson avec ${\delta=3 \;\;\lambda=1}$
```{r , include=TRUE}
Borne_Chernoff_Pois<- function(lambda,delta)
{
  borne= exp(-lambda*(1+(delta/lambda)*log(1+(delta/lambda))-(delta/lambda)));
  return (borne);
}
x1 = Borne_Chernoff_Pois(1,3)
r= rpois(10000,1)
x2=Prob_devia_sans_abs(10000,r,1,3);
list("Chernoff"=x1,"Monte Carlo"=x2) 
```

Quand le cas Poisson avec ${\delta=2 \;\;\lambda=2}$
```{r , include=TRUE}
x1 = Borne_Chernoff_Pois(2,2)
r= rpois(10000,2)
x2=Prob_devia_sans_abs(10000,r,2,2);
list("Chernoff"=x1,"Monte Carlo"=x2) 
```

### <--3-->

### 3.a

Remarque: 
$P(\bar X_n-\mu\ge \delta)  \le  exp(-\frac{n(\mu+\delta)^2}{2\sigma^2})$
```{r , include=TRUE}

# n est la fois d'échantillons
Borne_Chernoff_Gaus_n <- function(n,mu,sigma,delta)
{
  borne= exp(-n*((mu+delta)**2)/(2*(sigma**2)));
  return (borne);
}
Borne_Chernoff_Pois_n<- function(n,lambda,delta)
{
  borne= exp(n*(-lambda*(1+(delta/lambda)*log(1+(delta/lambda))-(delta/lambda))));
  return (borne);
}




x1=Borne_Chernoff_Gaus_n(20,0,1,1)
x2=Borne_Chernoff_Gaus_n(100,0,1,1)
x3=Borne_Chernoff_Gaus_n(1000,0,1,1)
y1=Borne_Chernoff_Pois_n(20,1,1)
y2=Borne_Chernoff_Pois_n(100,1,1)
y3=Borne_Chernoff_Pois_n(1000,1,1)
list("Gaus_20"=x1,"Gaus_100"=x2,"Gaus_1000"=x3)
list("Pois_20"=y1,"Pois_100"=y2,"Pois_1000"=y3)
```
### 3.b

estimateur de $\mu$ 
on met $\mu=la\; moyenne\;de\; Espérence \;empirique$ 
```{r , include=TRUE}
#
esti_de_mu <-function (N,mu,sigma)   
  { 
    sample<-array(1:N,c(N,20))
    moy <- 1:N
    for (i in 1:N) { 
        sample[i,] <-rnorm(20,mu,sigma)
        for(j in 1:20)
        {
          sum <- 0
          moy[i]= sum+sample[i,j]
        }
        moy[i]=moy[i]/20
    } 
    somme <- 0
    for(i in 1:N){
        somme=somme+moy[i]
    }
    return(somme/N)
} 
list("estimateur de mu quand N est 20"= esti_de_mu(20,0,1))
list("estimateur de mu quand N est 100"= esti_de_mu(100,0,1))
list("estimateur de mu quand N est 1000"= esti_de_mu(1000,0,1))
```
estimateur de $\lambda$
on met $\lambda= la\; moyenne\;de\; Espérence \;empirique$
```{r , include=TRUE}
esti_de_lambda <-function (N,lambda)   
  { 
    sample<-array(1:N,c(N,20))
    moy <- 1:N
    for (i in 1:N) { 
        sample[i,] <-rpois(20,lambda)
        for(j in 1:20)
        {
          sum <- 0
          moy[i]= sum+sample[i,j]
        }
        moy[i]=moy[i]/20
    } 
    somme <- 0
    for(i in 1:N){
        somme=somme+moy[i]
    }
    return(somme/N)
} 
list("estimateur de mu quand N est 20"= esti_de_lambda(20,1))
list("estimateur de mu quand N est 100"= esti_de_lambda(100,1))
list("estimateur de mu quand N est 1000"= esti_de_lambda(1000,1))
```
###Remarque: 
On peut remarquer que plus la fois d'échantillon est grande,plus l'estimateur est proche que le vrai espérence



### <--4-->

### 4.a
```{r , include=TRUE}
moyenne_empirique<- function(n,vec)
{
  sum <- 0
  for(i in vec)
    {
    sum=i+sum
  }
  return(sum/n)
}

for (n in c(20,100,1000,10000)){
  cauchy <- rcauchy(n,0,1)
  moy_emp <- moyenne_empirique(n,cauchy)
  cat("n=",n,"la moyenne empirique est",moy_emp,"\n")
}


```
La moyenne empirique ne converge pas selon "n"

### 4.b

on considère le “standard Cauchy distribution”,c'est à dire $\theta=0$, la fonction caractéristique sera $\Phi(t)=exp(-|t|)$, on trouve qu'il n'est pas dérivable en 0 ,c'est à dire son ésperence n'exsite pas . 

### 4.c
```{r , include=TRUE}
for(n in c(20,100,1000,10000)){
  cauchy <- rcauchy(n,0,1)
  s <- sort(cauchy)
  cat("la médiane de taille n=",n,"est",s[n/2+1],"\n")
}

```
on mets estimateur de $\theta=$ médiane 


